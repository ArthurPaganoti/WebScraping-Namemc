<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/__main__.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/__main__.py" />
              <option name="originalContent" value="import asyncio&#10;import requests&#10;import json&#10;from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError&#10;from bs4 import BeautifulSoup&#10;&#10;START_URL = &quot;https://pt.namemc.com/minecraft-names?offset=3196800&amp;sort=desc&quot;&#10;NEXT_PAGE_URL = &quot;https://pt.namemc.com/minecraft-names?sort=desc&amp;offset=25&quot;&#10;USER_DATA_DIR = &quot;./my_browser_session&quot;&#10;&#10;&#10;async def get_verified_session(url: str) -&gt; dict | None:&#10;&#10;    print(&quot; Lançando o navegador com configurações stealth manuais...&quot;)&#10;    try:&#10;        async with async_playwright() as p:&#10;            context = await p.chromium.launch_persistent_context(&#10;                user_data_dir=USER_DATA_DIR,&#10;                headless=False,&#10;                args=[&quot;--start-maximized&quot;],&#10;                no_viewport=True,&#10;            )&#10;            page = await context.new_page()&#10;            await page.add_init_script(&quot;Object.defineProperty(navigator, 'webdriver', {get: () =&gt; undefined})&quot;)&#10;            print(&quot;Navegando para o site...&quot;)&#10;            await page.goto(url, timeout=300000)&#10;            print(&quot;⏳ Aguardando Cloudflare ou o conteúdo...&quot;)&#10;            try:&#10;                await page.wait_for_selector('a[href^=&quot;/profile/&quot;]', timeout=10000)&#10;                print(&quot;✅ Conteúdo já presente. Nenhum desafio foi necessário.&quot;)&#10;            except PlaywrightTimeoutError:&#10;                print(&quot;Desafio detectado. Aguardando a resolução manual...&quot;)&#10;                print(&quot;Por favor, resolva o desafio do Cloudflare manualmente na janela aberta e aguarde...&quot;)&#10;                await page.wait_for_selector('a[href^=&quot;/profile/&quot;]', timeout=300000)&#10;                print(&quot;✅ Desafio do Cloudflare resolvido!&quot;)&#10;            cookies = await context.cookies()&#10;            user_agent = await page.evaluate(&quot;() =&gt; navigator.userAgent&quot;)&#10;            await context.close()&#10;            return {&quot;cookies&quot;: cookies, &quot;user_agent&quot;: user_agent}&#10;    except PlaywrightTimeoutError:&#10;        print(&quot;❌ O tempo limite foi atingido. A proteção do site pode ser muito forte ou o IP está sinalizado.&quot;)&#10;        return None&#10;    except Exception as e:&#10;        print(f&quot;Um erro inesperado ocorreu: {e}&quot;)&#10;        return None&#10;&#10;&#10;def scrape_and_get_data(session_data: dict, url: str) -&gt; list | None:&#10;&#10;    if not session_data:&#10;        print(&quot;Não é possível extrair dados, a sessão é inválida.&quot;)&#10;        return None&#10;&#10;    print(f&quot;\n⚙️  Preparando para extrair dados de {url} com a sessão verificada...&quot;)&#10;    s = requests.Session()&#10;    for cookie in session_data['cookies']:&#10;        s.cookies.set(cookie['name'], cookie['value'], domain=cookie['domain'])&#10;    s.headers.update({&quot;User-Agent&quot;: session_data['user_agent']})&#10;&#10;    try:&#10;        response = s.get(url)&#10;        response.raise_for_status()&#10;        print(f&quot;✔️ Página buscada com sucesso, status: {response.status_code}&quot;)&#10;&#10;        with open('pagina_bruta.html', 'w', encoding='utf-8') as f:&#10;            f.write(response.text)&#10;&#10;        soup = BeautifulSoup(response.text, 'html.parser')&#10;        scraped_data = []&#10;&#10;        for row in soup.find_all('tr'):&#10;            cols = row.find_all('td')&#10;            if len(cols) &gt;= 2:&#10;                name = cols[0].get_text(strip=True)&#10;                drop_time = cols[1].get_text(strip=True)&#10;                if name and drop_time:&#10;                    scraped_data.append({&#10;                        &quot;name&quot;: name,&#10;                        &quot;drop_time&quot;: drop_time&#10;                    })&#10;&#10;        if not scraped_data:&#10;            for div in soup.find_all('div', class_='d-table-row'):&#10;                name = div.find('a', class_='text-ellipsis')&#10;                drop_time = div.find('div', class_='pt-0 text-left text-ellipsis')&#10;                if name and drop_time:&#10;                    scraped_data.append({&#10;                        &quot;name&quot;: name.get_text(strip=True),&#10;                        &quot;drop_time&quot;: drop_time.get_text(strip=True)&#10;                    })&#10;&#10;        return scraped_data if scraped_data else None&#10;&#10;    except requests.RequestException as e:&#10;        print(f&quot;❌ Falha ao extrair dados com a sessão: {e}&quot;)&#10;        return None&#10;&#10;&#10;async def main():&#10;&#10;    session_data = await get_verified_session(START_URL)&#10;    if session_data:&#10;        scraped_data = scrape_and_get_data(session_data, START_URL)&#10;&#10;        if scraped_data:&#10;            output_filename = 'minecraft_names.json'&#10;            with open(output_filename, 'w', encoding='utf-8') as f:&#10;                json.dump(scraped_data, f, ensure_ascii=False, indent=4)&#10;            print(f&quot;\n✨ Operação concluída! Foram salvos {len(scraped_data)} nomes no arquivo '{output_filename}'.&quot;)&#10;        else:&#10;            print(&quot;❌ Nenhum dado foi extraído da página. Verifique o arquivo 'pagina_bruta.html' para depuração.&quot;)&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    asyncio.run(main())" />
              <option name="updatedContent" value="import asyncio&#10;import requests&#10;import json&#10;from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError&#10;from bs4 import BeautifulSoup&#10;&#10;START_URL = &quot;https://pt.namemc.com/minecraft-names?offset=3196800&amp;sort=desc&quot;&#10;NEXT_PAGE_URL = &quot;https://pt.namemc.com/minecraft-names?sort=desc&amp;offset=25&quot;&#10;USER_DATA_DIR = &quot;./my_browser_session&quot;&#10;&#10;&#10;async def get_verified_session(url: str) -&gt; dict | None:&#10;&#10;    print(&quot; Lançando o navegador com configurações stealth manuais...&quot;)&#10;    try:&#10;        async with async_playwright() as p:&#10;            context = await p.chromium.launch_persistent_context(&#10;                user_data_dir=USER_DATA_DIR,&#10;                headless=False,&#10;                args=[&quot;--start-maximized&quot;],&#10;                no_viewport=True,&#10;            )&#10;            page = await context.new_page()&#10;            await page.add_init_script(&quot;Object.defineProperty(navigator, 'webdriver', {get: () =&gt; undefined})&quot;)&#10;            print(&quot;Navegando para o site...&quot;)&#10;            await page.goto(url, timeout=300000)&#10;            print(&quot;⏳ Aguardando Cloudflare ou o conteúdo...&quot;)&#10;            try:&#10;                await page.wait_for_selector('a[href^=&quot;/profile/&quot;]', timeout=10000)&#10;                print(&quot;✅ Conteúdo já presente. Nenhum desafio foi necessário.&quot;)&#10;            except PlaywrightTimeoutError:&#10;                print(&quot;Desafio detectado. Aguardando a resolução manual...&quot;)&#10;                print(&quot;Por favor, resolva o desafio do Cloudflare manualmente na janela aberta e aguarde...&quot;)&#10;                await page.wait_for_selector('a[href^=&quot;/profile/&quot;]', timeout=300000)&#10;                print(&quot;✅ Desafio do Cloudflare resolvido!&quot;)&#10;            cookies = await context.cookies()&#10;            user_agent = await page.evaluate(&quot;() =&gt; navigator.userAgent&quot;)&#10;            await context.close()&#10;            return {&quot;cookies&quot;: cookies, &quot;user_agent&quot;: user_agent}&#10;    except PlaywrightTimeoutError:&#10;        print(&quot;❌ O tempo limite foi atingido. A proteção do site pode ser muito forte ou o IP está sinalizado.&quot;)&#10;        return None&#10;    except Exception as e:&#10;        print(f&quot;Um erro inesperado ocorreu: {e}&quot;)&#10;        return None&#10;&#10;&#10;def scrape_and_get_data(session_data: dict, url: str) -&gt; list | None:&#10;&#10;    if not session_data:&#10;        print(&quot;Não é possível extrair dados, a sessão é inválida.&quot;)&#10;        return None&#10;&#10;    print(f&quot;\n⚙️  Preparando para extrair dados de {url} com a sessão verificada...&quot;)&#10;    s = requests.Session()&#10;    for cookie in session_data['cookies']:&#10;        s.cookies.set(cookie['name'], cookie['value'], domain=cookie['domain'])&#10;    s.headers.update({&quot;User-Agent&quot;: session_data['user_agent']})&#10;&#10;    try:&#10;        response = s.get(url)&#10;        response.raise_for_status()&#10;        print(f&quot;✔️ Página buscada com sucesso, status: {response.status_code}&quot;)&#10;&#10;        with open('pagina_bruta.html', 'w', encoding='utf-8') as f:&#10;            f.write(response.text)&#10;&#10;        soup = BeautifulSoup(response.text, 'html.parser')&#10;        scraped_data = []&#10;&#10;        # Tenta encontrar a tabela principal&#10;        table = soup.find('table')&#10;        if table:&#10;            print(&quot;Tabela encontrada!&quot;)&#10;            rows = table.find_all('tr')&#10;            print(f&quot;Total de linhas na tabela: {len(rows)}&quot;)&#10;            # Salva um trecho da tabela para depuração&#10;            with open('debug_tabela.html', 'w', encoding='utf-8') as f:&#10;                f.write(str(table)[:5000])&#10;            for row in rows:&#10;                cols = row.find_all('td')&#10;                if len(cols) &gt;= 2:&#10;                    name = cols[0].get_text(strip=True)&#10;                    drop_time = cols[1].get_text(strip=True)&#10;                    if name and drop_time:&#10;                        scraped_data.append({&#10;                            &quot;name&quot;: name,&#10;                            &quot;drop_time&quot;: drop_time&#10;                        })&#10;        else:&#10;            print(&quot;Tabela não encontrada. Tentando buscar por divs responsivos...&quot;)&#10;            divs = soup.find_all('div', class_='d-table-row')&#10;            print(f&quot;Total de divs d-table-row: {len(divs)}&quot;)&#10;            with open('debug_tabela.html', 'w', encoding='utf-8') as f:&#10;                for div in divs[:10]:&#10;                    f.write(str(div))&#10;                    f.write('\n---\n')&#10;            for div in divs:&#10;                name = div.find('a', class_='text-ellipsis')&#10;                drop_time = div.find('div', class_='pt-0 text-left text-ellipsis')&#10;                if name and drop_time:&#10;                    scraped_data.append({&#10;                        &quot;name&quot;: name.get_text(strip=True),&#10;                        &quot;drop_time&quot;: drop_time.get_text(strip=True)&#10;                    })&#10;&#10;        return scraped_data if scraped_data else None&#10;&#10;    except requests.RequestException as e:&#10;        print(f&quot;❌ Falha ao extrair dados com a sessão: {e}&quot;)&#10;        return None&#10;&#10;&#10;async def main():&#10;&#10;    session_data = await get_verified_session(START_URL)&#10;    if session_data:&#10;        scraped_data = scrape_and_get_data(session_data, START_URL)&#10;&#10;        if scraped_data:&#10;            output_filename = 'minecraft_names.json'&#10;            with open(output_filename, 'w', encoding='utf-8') as f:&#10;                json.dump(scraped_data, f, ensure_ascii=False, indent=4)&#10;            print(f&quot;\n✨ Operação concluída! Foram salvos {len(scraped_data)} nomes no arquivo '{output_filename}'.&quot;)&#10;        else:&#10;            print(&quot;❌ Nenhum dado foi extraído da página. Verifique o arquivo 'pagina_bruta.html' para depuração.&quot;)&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    asyncio.run(main())" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>